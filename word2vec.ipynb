{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "079d7817-0585-4729-acfd-e42f5c10c4c2",
      "metadata": {
        "id": "079d7817-0585-4729-acfd-e42f5c10c4c2",
        "outputId": "63fc7600-a4fb-41d4-8d9a-6ff396d8da19",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]   Package reuters is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('reuters')\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "import time\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import logging\n",
        "from nltk.corpus import reuters\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = []\n",
        "for fileid in reuters.fileids():\n",
        "    tokens.extend([word.lower() for word in reuters.words(fileid)])\n",
        "\n",
        "tokens = tokens[:100_000]\n",
        "print(f\"number of tokens: {len(tokens)}\")\n",
        "print(f\"sample of tokens: {tokens[:20]}\")"
      ],
      "metadata": {
        "id": "L20nh2nXt7GO",
        "outputId": "4a657150-2b6c-4141-cdb5-8b2fe6f500db",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "L20nh2nXt7GO",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of tokens: 100000\n",
            "sample of tokens: ['asian', 'exporters', 'fear', 'damage', 'from', 'u', '.', 's', '.-', 'japan', 'rift', 'mounting', 'trade', 'friction', 'between', 'the', 'u', '.', 's', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "89ab9a23-671e-4ee2-a6fa-e444576f5128",
      "metadata": {
        "id": "89ab9a23-671e-4ee2-a6fa-e444576f5128"
      },
      "outputs": [],
      "source": [
        "class Word2Vec:\n",
        "    def __init__(self, embedding_dim=100, window_size=3, min_count=5,\n",
        "                 batch_size=2048, epochs=3, learning_rate=0.001):\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.window_size = window_size\n",
        "        self.min_count = min_count\n",
        "        self.batch_size = batch_size\n",
        "        self.epochs = epochs\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "        self.word_to_ix = None\n",
        "        self.ix_to_word = None\n",
        "        self.model = None\n",
        "\n",
        "    def build_vocab(self, tokens):\n",
        "        # build vocabulary from tokens with minimum frequency threshold\n",
        "        word_counts = Counter(tokens)\n",
        "        vocab = [word for word, count in word_counts.items()\n",
        "                if count >= self.min_count]\n",
        "\n",
        "        self.word_to_ix = {word: i for i, word in enumerate(['<UNK>'] + vocab)}\n",
        "        self.ix_to_word = {i: word for word, i in self.word_to_ix.items()}\n",
        "\n",
        "        logger.info(f\"vocabulary size: {len(self.word_to_ix)}\")\n",
        "        return vocab\n",
        "\n",
        "    def subsample_frequent_words(self, tokens, threshold=1e-5):\n",
        "        # subsample frequent words using paper's formula\n",
        "        word_counts = Counter(tokens)\n",
        "        total_count = len(tokens)\n",
        "        word_freq = {word: count/total_count for word, count in word_counts.items()}\n",
        "\n",
        "        prob_drop = {word: 1 - np.sqrt(threshold/freq)\n",
        "                    for word, freq in word_freq.items()}\n",
        "\n",
        "        return [token for token in tokens\n",
        "                if np.random.random() > prob_drop[token]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "ade12ebb-3b9f-4abe-9157-5cae2e0cb039",
      "metadata": {
        "id": "ade12ebb-3b9f-4abe-9157-5cae2e0cb039"
      },
      "outputs": [],
      "source": [
        "class SkipGramDataset(Dataset):\n",
        "    def __init__(self, tokens, word_to_ix, window_size=3):\n",
        "        self.tokens = tokens\n",
        "        self.word_to_ix = word_to_ix\n",
        "        self.window_size = window_size\n",
        "        self.pairs = self._generate_pairs()\n",
        "\n",
        "    def _generate_pairs(self):\n",
        "        pairs = []\n",
        "        for i in range(len(self.tokens)):\n",
        "            start = max(0, i - self.window_size)\n",
        "            end = min(len(self.tokens), i + self.window_size + 1)\n",
        "            context = self.tokens[start:i] + self.tokens[i+1:end]\n",
        "            pairs.extend([(self.tokens[i], ctx) for ctx in context])\n",
        "        return pairs\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.pairs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        target, context = self.pairs[idx]\n",
        "        target_ix = self.word_to_ix.get(target, self.word_to_ix['<UNK>'])\n",
        "        context_ix = self.word_to_ix.get(context, self.word_to_ix['<UNK>'])\n",
        "        return torch.tensor(target_ix), torch.tensor(context_ix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "db406633-4aaa-4fb3-a7b7-31227b1d39c8",
      "metadata": {
        "id": "db406633-4aaa-4fb3-a7b7-31227b1d39c8"
      },
      "outputs": [],
      "source": [
        "class SkipGramModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim):\n",
        "        super(SkipGramModel, self).__init__()\n",
        "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.output = nn.Linear(embedding_dim, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded = self.embeddings(x)\n",
        "        out = self.output(embedded)\n",
        "        return torch.log_softmax(out, dim=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "7da5872b-05dd-4672-b0fa-69046fceb1f9",
      "metadata": {
        "id": "7da5872b-05dd-4672-b0fa-69046fceb1f9"
      },
      "outputs": [],
      "source": [
        "class Word2VecTrainer:\n",
        "    def __init__(self, model, dataset, learning_rate=0.001):\n",
        "        self.model = model\n",
        "        self.dataset = dataset\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "    def train(self, epochs, batch_size):\n",
        "        dataloader = DataLoader(self.dataset, batch_size=batch_size, shuffle=True)\n",
        "        optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
        "        criterion = nn.NLLLoss()\n",
        "\n",
        "        start_time = time.time()\n",
        "        for epoch in range(epochs):\n",
        "            total_loss = 0\n",
        "            for batch_idx, (target, context) in enumerate(dataloader):\n",
        "                log_probs = self.model(target)\n",
        "                loss = criterion(log_probs, context)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                total_loss += loss.item()\n",
        "\n",
        "                if batch_idx % 100 == 0:\n",
        "                    logger.info(f\"epoch {epoch}, batch {batch_idx}, \"\n",
        "                              f\"loss: {loss.item():.4f}\")\n",
        "\n",
        "            avg_loss = total_loss / len(dataloader)\n",
        "            logger.info(f\"epoch {epoch} completed, avg loss: {avg_loss:.4f}\")\n",
        "\n",
        "        training_time = time.time() - start_time\n",
        "        logger.info(f\"training completed in {training_time:.2f} seconds\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "b2a9951d-0cc1-4c19-8125-2ab03ce247d4",
      "metadata": {
        "id": "b2a9951d-0cc1-4c19-8125-2ab03ce247d4",
        "outputId": "ef70460e-e6ee-4217-bf9a-751347d45d22",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total training pairs: 599988\n"
          ]
        }
      ],
      "source": [
        "w2v = Word2Vec(\n",
        "    embedding_dim=100,\n",
        "    window_size=3,\n",
        "    min_count=5,\n",
        "    batch_size=2048,\n",
        "    epochs=3\n",
        ")\n",
        "\n",
        "vocab = w2v.build_vocab(tokens)\n",
        "\n",
        "dataset = SkipGramDataset(tokens, w2v.word_to_ix, window_size=w2v.window_size)\n",
        "print(f\"Total training pairs: {len(dataset)}\")\n",
        "\n",
        "model = SkipGramModel(len(w2v.word_to_ix), w2v.embedding_dim)\n",
        "\n",
        "trainer = Word2VecTrainer(model, dataset, learning_rate=w2v.learning_rate)\n",
        "trainer.train(epochs=w2v.epochs, batch_size=w2v.batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Word2VecEvaluator:\n",
        "    def __init__(self, model, word_to_ix, ix_to_word):\n",
        "        self.model = model\n",
        "        self.word_to_ix = word_to_ix\n",
        "        self.ix_to_word = ix_to_word\n",
        "\n",
        "    def similar_words(self, word, n=5):\n",
        "        if word not in self.word_to_ix:\n",
        "            return []\n",
        "\n",
        "        word_ix = torch.tensor([self.word_to_ix[word]])\n",
        "        word_vec = self.model.embeddings(word_ix).detach().numpy()[0]\n",
        "\n",
        "        similarities = []\n",
        "        for w, ix in self.word_to_ix.items():\n",
        "            if w != word and w != '<UNK>':\n",
        "                vec = self.model.embeddings(torch.tensor([ix])).detach().numpy()[0]\n",
        "                similarity = np.dot(word_vec, vec) / (np.linalg.norm(word_vec) * np.linalg.norm(vec))\n",
        "                similarities.append((w, similarity))\n",
        "\n",
        "        return sorted(similarities, key=lambda x: x[1], reverse=True)[:n]\n",
        "\n",
        "evaluator = Word2VecEvaluator(model, w2v.word_to_ix, w2v.ix_to_word)\n",
        "\n",
        "test_words = ['trade', 'oil', 'bank', 'market']\n",
        "for word in test_words:\n",
        "    if word in w2v.word_to_ix:\n",
        "        print(f\"\\nsimilar to '{word}':\")\n",
        "        similar = evaluator.similar_words(word, n=5)\n",
        "        for w, score in similar:\n",
        "            print(f\"{w}: {score:.3f}\")"
      ],
      "metadata": {
        "id": "kda106uqw_Nr",
        "outputId": "541dbf8f-3e09-45a6-d160-22c119aa696c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "kda106uqw_Nr",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "similar to 'trade':\n",
            "required: 0.415\n",
            "257: 0.339\n",
            "account: 0.330\n",
            "french: 0.320\n",
            "amount: 0.313\n",
            "\n",
            "similar to 'oil':\n",
            "spot: 0.351\n",
            "telephone: 0.339\n",
            "fund: 0.333\n",
            "pilots: 0.322\n",
            "plan: 0.317\n",
            "\n",
            "similar to 'bank':\n",
            "sectors: 0.330\n",
            "lawson: 0.323\n",
            "industry: 0.310\n",
            "record: 0.307\n",
            "general: 0.300\n",
            "\n",
            "similar to 'market':\n",
            "21: 0.356\n",
            "holding: 0.323\n",
            "turkey: 0.311\n",
            "council: 0.309\n",
            "just: 0.302\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.17"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}