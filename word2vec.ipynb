{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "079d7817-0585-4729-acfd-e42f5c10c4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import urllib.request\n",
    "import os\n",
    "import time\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "89ab9a23-671e-4ee2-a6fa-e444576f5128",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2Vec:\n",
    "    def __init__(self, embedding_dim=300, window_size=5, min_count=5, \n",
    "                 batch_size=512, epochs=5, learning_rate=0.001):\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.window_size = window_size\n",
    "        self.min_count = min_count\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs = epochs\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        self.word_to_ix = None\n",
    "        self.ix_to_word = None\n",
    "        self.model = None\n",
    "        \n",
    "    def download_text8(self):\n",
    "        \"\"\"Download the text8 dataset if not already present\"\"\"\n",
    "        if not os.path.exists('text8'):\n",
    "            logger.info(\"Downloading text8 dataset...\")\n",
    "            url = 'http://mattmahoney.net/dc/text8.zip'\n",
    "            urllib.request.urlretrieve(url, 'text8.zip')\n",
    "            os.system('unzip text8.zip')\n",
    "            logger.info(\"Download complete\")\n",
    "    \n",
    "    def preprocess_text(self, text, max_tokens=None):\n",
    "        \"\"\"Split text into tokens\"\"\"\n",
    "        tokens = text.split()\n",
    "        return tokens[:max_tokens] if max_tokens else tokens\n",
    "    \n",
    "    def build_vocab(self, tokens):\n",
    "        \"\"\"Build vocabulary from tokens with minimum frequency threshold\"\"\"\n",
    "        word_counts = Counter(tokens)\n",
    "        vocab = [word for word, count in word_counts.items() \n",
    "                if count >= self.min_count]\n",
    "        \n",
    "        self.word_to_ix = {word: i for i, word in enumerate(['<UNK>'] + vocab)}\n",
    "        self.ix_to_word = {i: word for word, i in self.word_to_ix.items()}\n",
    "        \n",
    "        logger.info(f\"Vocabulary size: {len(self.word_to_ix)}\")\n",
    "        return vocab\n",
    "    \n",
    "    def subsample_frequent_words(self, tokens, threshold=1e-5):\n",
    "        \"\"\"Subsample frequent words using Word2Vec paper's formula\"\"\"\n",
    "        word_counts = Counter(tokens)\n",
    "        total_count = len(tokens)\n",
    "        word_freq = {word: count/total_count for word, count in word_counts.items()}\n",
    "        \n",
    "        prob_drop = {word: 1 - np.sqrt(threshold/freq) \n",
    "                    for word, freq in word_freq.items()}\n",
    "        \n",
    "        return [token for token in tokens \n",
    "                if np.random.random() > prob_drop[token]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ade12ebb-3b9f-4abe-9157-5cae2e0cb039",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGramDataset(Dataset):\n",
    "    def __init__(self, tokens, word_to_ix, window_size=5):\n",
    "        self.tokens = tokens\n",
    "        self.word_to_ix = word_to_ix\n",
    "        self.window_size = window_size\n",
    "        self.pairs = self._generate_pairs()\n",
    "    \n",
    "    def _generate_pairs(self):\n",
    "        pairs = []\n",
    "        for i in range(len(self.tokens)):\n",
    "            start = max(0, i - self.window_size)\n",
    "            end = min(len(self.tokens), i + self.window_size + 1)\n",
    "            context = self.tokens[start:i] + self.tokens[i+1:end]\n",
    "            pairs.extend([(self.tokens[i], ctx) for ctx in context])\n",
    "        return pairs\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        target, context = self.pairs[idx]\n",
    "        target_ix = self.word_to_ix.get(target, self.word_to_ix['<UNK>'])\n",
    "        context_ix = self.word_to_ix.get(context, self.word_to_ix['<UNK>'])\n",
    "        return torch.tensor(target_ix), torch.tensor(context_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "db406633-4aaa-4fb3-a7b7-31227b1d39c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Model Class\n",
    "class SkipGramModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(SkipGramModel, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.output = nn.Linear(embedding_dim, vocab_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        embedded = self.embeddings(x)\n",
    "        out = self.output(embedded)\n",
    "        return torch.log_softmax(out, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7da5872b-05dd-4672-b0fa-69046fceb1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Trainer Class\n",
    "class Word2VecTrainer:\n",
    "    def __init__(self, model, dataset, learning_rate=0.001):\n",
    "        self.model = model\n",
    "        self.dataset = dataset\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "    def train(self, epochs, batch_size):\n",
    "        dataloader = DataLoader(self.dataset, batch_size=batch_size, shuffle=True)\n",
    "        optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "        criterion = nn.NLLLoss()\n",
    "        \n",
    "        start_time = time.time()\n",
    "        for epoch in range(epochs):\n",
    "            total_loss = 0\n",
    "            for batch_idx, (target, context) in enumerate(dataloader):\n",
    "                log_probs = self.model(target)\n",
    "                loss = criterion(log_probs, context)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "                \n",
    "                if batch_idx % 100 == 0:\n",
    "                    logger.info(f\"Epoch {epoch}, Batch {batch_idx}, \"\n",
    "                              f\"Loss: {loss.item():.4f}\")\n",
    "            \n",
    "            avg_loss = total_loss / len(dataloader)\n",
    "            logger.info(f\"Epoch {epoch} completed, Average Loss: {avg_loss:.4f}\")\n",
    "        \n",
    "        training_time = time.time() - start_time\n",
    "        logger.info(f\"Training completed in {training_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b2a9951d-0cc1-4c19-8125-2ab03ce247d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2VecEvaluator:\n",
    "    def __init__(self, model, word_to_ix, ix_to_word):\n",
    "        self.model = model\n",
    "        self.word_to_ix = word_to_ix\n",
    "        self.ix_to_word = ix_to_word\n",
    "        \n",
    "    def get_vector(self, word):\n",
    "        \"\"\"Get the embedding vector for a word\"\"\"\n",
    "        if word not in self.word_to_ix:\n",
    "            return None\n",
    "        word_ix = torch.tensor([self.word_to_ix[word]])\n",
    "        return self.model.embeddings(word_ix).detach().numpy()[0]\n",
    "    \n",
    "    def similar_words(self, word, n=5):\n",
    "        \"\"\"Find n most similar words\"\"\"\n",
    "        if word not in self.word_to_ix:\n",
    "            return []\n",
    "        \n",
    "        target_vec = self.get_vector(word)\n",
    "        similarities = []\n",
    "        \n",
    "        for w in self.word_to_ix:\n",
    "            if w != word and w != '<UNK>':\n",
    "                vec = self.get_vector(w)\n",
    "                similarity = float(np.dot(target_vec, vec) / \n",
    "                                (np.linalg.norm(target_vec) * np.linalg.norm(vec)))\n",
    "                similarities.append((w, similarity))\n",
    "        \n",
    "        return sorted(similarities, key=lambda x: x[1], reverse=True)[:n]\n",
    "    \n",
    "    def get_analogy(self, word1, word2, word3, n=5):\n",
    "        \"\"\"Find word4 such that word1:word2 :: word3:word4\"\"\"\n",
    "        for word in [word1, word2, word3]:\n",
    "            if word not in self.word_to_ix:\n",
    "                return []\n",
    "        \n",
    "        v1, v2, v3 = map(self.get_vector, [word1, word2, word3])\n",
    "        target = v2 - v1 + v3\n",
    "        \n",
    "        similarities = []\n",
    "        for w in self.word_to_ix:\n",
    "            if w not in [word1, word2, word3, '<UNK>']:\n",
    "                vec = self.get_vector(w)\n",
    "                similarity = np.dot(target, vec) / (np.linalg.norm(target) * \n",
    "                                                  np.linalg.norm(vec))\n",
    "                similarities.append((w, similarity))\n",
    "        \n",
    "        return sorted(similarities, key=lambda x: x[1], reverse=True)[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6574d7ce-9a2a-4ec4-9827-144bb1b46789",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Vocabulary size: 13967\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens: 1000000\n",
      "Total training pairs: 9999970\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Epoch 0, Batch 0, Loss: 9.7407\n",
      "INFO:__main__:Epoch 0, Batch 100, Loss: 8.5167\n",
      "INFO:__main__:Epoch 0, Batch 200, Loss: 7.9896\n",
      "INFO:__main__:Epoch 0, Batch 300, Loss: 7.6763\n",
      "INFO:__main__:Epoch 0, Batch 400, Loss: 7.4904\n",
      "INFO:__main__:Epoch 0, Batch 500, Loss: 7.2885\n",
      "INFO:__main__:Epoch 0, Batch 600, Loss: 7.4762\n",
      "INFO:__main__:Epoch 0, Batch 700, Loss: 6.9735\n",
      "INFO:__main__:Epoch 0, Batch 800, Loss: 7.0200\n",
      "INFO:__main__:Epoch 0, Batch 900, Loss: 7.3512\n",
      "INFO:__main__:Epoch 0, Batch 1000, Loss: 7.0595\n",
      "INFO:__main__:Epoch 0, Batch 1100, Loss: 7.1793\n",
      "INFO:__main__:Epoch 0, Batch 1200, Loss: 7.1391\n",
      "INFO:__main__:Epoch 0, Batch 1300, Loss: 6.8866\n",
      "INFO:__main__:Epoch 0, Batch 1400, Loss: 7.1409\n",
      "INFO:__main__:Epoch 0, Batch 1500, Loss: 7.1071\n",
      "INFO:__main__:Epoch 0, Batch 1600, Loss: 6.9509\n",
      "INFO:__main__:Epoch 0, Batch 1700, Loss: 6.8570\n",
      "INFO:__main__:Epoch 0, Batch 1800, Loss: 6.7480\n",
      "INFO:__main__:Epoch 0, Batch 1900, Loss: 6.8451\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 26\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Train model\u001b[39;00m\n\u001b[1;32m     25\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Word2VecTrainer(model, dataset, learning_rate\u001b[38;5;241m=\u001b[39mw2v\u001b[38;5;241m.\u001b[39mlearning_rate)\n\u001b[0;32m---> 26\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mw2v\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mw2v\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[26], line 21\u001b[0m, in \u001b[0;36mWord2VecTrainer.train\u001b[0;34m(self, epochs, batch_size)\u001b[0m\n\u001b[1;32m     18\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(log_probs, context)\n\u001b[1;32m     20\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 21\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     24\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Initialize Word2Vec\n",
    "w2v = Word2Vec(embedding_dim=300, window_size=5, min_count=5, \n",
    "               batch_size=512, epochs=5)\n",
    "\n",
    "# Download and load data\n",
    "w2v.download_text8()\n",
    "with open('text8', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# Preprocess text (using first 1M tokens for quick testing)\n",
    "tokens = w2v.preprocess_text(text, max_tokens=1_000_000)\n",
    "print(f\"Total tokens: {len(tokens)}\")\n",
    "\n",
    "# Build vocabulary\n",
    "vocab = w2v.build_vocab(tokens)\n",
    "\n",
    "# Create dataset\n",
    "dataset = SkipGramDataset(tokens, w2v.word_to_ix, window_size=w2v.window_size)\n",
    "print(f\"Total training pairs: {len(dataset)}\")\n",
    "\n",
    "# Initialize model\n",
    "model = SkipGramModel(len(w2v.word_to_ix), w2v.embedding_dim)\n",
    "\n",
    "# Train model\n",
    "trainer = Word2VecTrainer(model, dataset, learning_rate=w2v.learning_rate)\n",
    "trainer.train(epochs=w2v.epochs, batch_size=w2v.batch_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
